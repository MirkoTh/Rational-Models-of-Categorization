
Rational models of categorization
=================================

Here are implemented several versions of the so-called rational model of
categorization. Executing any of the python files here will launch a test/demo
of one of these classic models.

All these models assume that stimuli are generated by a mixture of underlying
distributions of unknown size. These distributions are Gaussian on continuous
stimulus dimensions, and multinomial on discrete stimulus dimensions. The prior
over the number of mixture components is given via a Dirichlet distribution.

They bear a strong resemblance to naive Bayes.


The original: rational.py
-------------------------
Anderson's original model is available here. The demo performs the task that is
walked through in Figure 1 in [Anderson (1991)]_, in which he runs the model on
the classic [Medin & Schaffer (1978)]_ task.

Becuase this model was developed before advanced techniques for approximating
intractable Bayesian posteriors were in wide use, the model views stimuli
sequentially and assigns them deterministically to the cluster that was most
likely to have generated them.

.. [Anderson (1991)] Anderon, J. R. (1991). "The adaptive nature of human
   categorization." Psychological Review, 98:409-429.

.. [Medin & Schaffer (1978)] Medin, D. L. and Schaffer, M. M. (1978). "Context
   Theory of Classification Learning." Psychological Review, 85:207-238.


The "more rational" model.
--------------------------
Recently, Sanborn, Griffiths and Navarro [(2006)]_ brought the model up to date
with two methods for approximating the full posterior over possible partitions
of the stimuli. I have implemented the basic model in the document particle.py,
and implemented each of these approximation methods as extensions. Since the
Anderson model is a special use of the "more rational" model, it can also be
run here.

Gibbs Sampling 
    All items are assigned arbitrarily. Sampling proceeds by removing each item
    one by one and relabeling it probabilistically. In the limit, the
    likelihood of a given partition of the stimuli is given by the number of
    times it is visited. I have implemented this in GibbsSampler.py.
    
Particle filtering.
    Items are viewed sequentially, as in the Anderson (1991) model, but the
    model tracks many hypotheses about the correct partition, and at each stage
    resamples from its own existing samples. I have implemented this in
    filter.py.

.. [(2006)] Sanborn, A. N., Griffiths T. L., and Navarro, D. J. (2006)
   "A More Rational Model of Categorization." Proceedings of the 28th Annual
   Conference of the Cognitive Science Society.
